###################### XEN virtualization ########################

One of the most widely used virtualization platforms. Installation of xen includes the hypervisor and kernel.
Selecting the xen kernel will launch a virtual machine called Dom0(domain0) which has system level privileges.
Gues OS's are referred to as DomU (unprivileged domains).

#Storage:

Enough space is needed for Dom0 and for gues OS disk images.
Use lVM to create multiple block devices or use a local filesystem.
During host operating system install select LVM for file systems.

#Networking 

Default and most common network setup is bridged.

#Setup network bridge

1. Disable network manager
systemctl stop NetworkManager && systemctl disable NetworkManager

2. Create bridge on Dom0 called xenbr0 (Ubuntu/Debian)
apt-get install bridge-utils

3. Update network interfaces

auto lo
iface lo inet loppback

auto xenbr0
iface xenbr0 inet dhcp
bridge_ports eth0

#### Xen installation 

#rhel/centos

1. Make sure that centos-extras repo is installed
2. install required packages
yum install centos-release-xen && yum update

3. install xen kernel/hypervisor
yum install xen

4. reboot into the new xen kernel
Note: for some features virtualization package may need to be installed
yum groupinstall 'Virtualization'


#ubuntu/debian

1. Install xen kernel/hypervisor
apt-get install xen-system-amd64  - ubuntu
apt-get install xen-linux-system  - debian

2. Boot into xen kernel

## commands

#list domains 

xl list
xl info
xl top

## Boot configuration

#ubuntu
1. locate the name of the menu entry for xen hypervisor
grep menuentry /boot/grup/grub.cfg

2. Add the name of the entry to /etc/default/grub
Change GRUB_DEFAULT=0 to
GRUB_DEFAULT="Ubuntu GNU/Linux with Xen hypervisor"   - example

3. Update grub config
update-grub

#Centos
1,2  -  the same

3. Update grub
grub2-mkconfig -o /boot/grub2/grub.cfg

#for UEFI
grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg

#Parameters can be added to grub config to alter xen during boot , example restrit cpu and memory for Dom0
1. for grub1
add kernel argument in /boot/grub/grub.cfg  (dom0_max-vcpus=1 and dom0_mem=512M,max=512M)
2. for grub2
Add/Edit line in /etc/default/grub
GRUB_CMDLINE_XEN_DEFAULT="dom0_max-vcpus=1 dom0_mem=512M,max=512M"  , then run update-grub

## Xen configuration

/etc/xen

Location for Domain.cfg which are used to create guests
conatains sample gues config files ( for PV and HVM)
location of xl.conf which is the global config file for xl toolstack

/etc/default/
xen - allows you to set the default toolstack for the installation
grub - allows to set xen kernel as default

/etc/default/xendomains
configure guest behaviour on host reboot
config xen to save running domains when dom0 is shutdown
config to auto start saved domains when system is started

/etc/xen/auto
place symlinks to guest .cfg files so that they can start on boot
/var/lib/xen
stores files containing user data for guests

## Xen utilities

#xen-tools

# debian/ubuntu install
apt-get install xen-tools

# centos install

yum install debootstrap perl-Text-Templateh perl-Config-IniFiles perl-File-Slurp perl-File-Which perl-Data-Dumper 
wget http://xen-tools.org/software/xen-tools/xen-tools-4.3.1.tar.gz
untar the archive
make install

# Primary commands
xen-create-image    --  allows you to create new xen guest domains
xen-list-images
xen-delete-image

config files /etc/xen-tools/xen-tools.conf
allows for defaults when creating guests

#example guest install
xen-create-image --hostname=lpic --lvm=ubuntu-vg --vcpus=1 --memory=256M --maxmem=256M --size=8G --swap=1000m --dhcp --dist=jessie



##################### KVM virtualization #####################

KVM - full virtualization solution for linux on x86 hardware that takes advantage of the intel and amd extensions
QEMU - open source machine emulator and also functions as virtualizer when paired with KVM or XEN
Libvirt - collection of software that provides unified way to manage VMs , it consists of an API library, a daemon(libvirtd) and CLI utility (virsh)


## KVM installation

1. Load kernel modules if not loaded
modprobe kvm
modprobe kvm_intel (for Intel)
modprobe kvm_amd   (for AMD)

2. Install qemu-kvm and required packages:
yum update && yum install qemu-kvm
yum install libvirt virt-install virt-viewer

3. Start and enable libvirtd


## Deploy VM
1. Create disk image
qemu-img create -f qcow2 /var/lib/libvirt/images/centos7.img 10G

2. Create VM
qemu-kvm -name centos -hda /var/lib/libvirt/images/centos7.img -cdrom /path/to/intall/iso -boot d -m 1024M

## deploy VM using virt-install
virt-install --name=centos --memory=1024 --vcpus=1 --disk=8 --os-variant=centos7.0 --cdrom /path/to/install/iso



##################### LVS install and configure #####################

# Ref: http://kb.linuxvirtualserver.org/wiki/Ipvsadm

Linux virtual server is a highly available and highly scalable server that is built on a cluster of real servers using a load balancer
Types of load balanced clusters:

DNS based - it uses DNS to distribute requests to members of the cluster
dispatcher - it uses a a dispatcher , also known as load balancer to distribute requests to members of the cluster
KTCPVS(Kernel TCP virtual server) - application level load balancing inside the linux kernel. the content of the reuqests is known before it is send to the real servers
IPVS(IP virtual server) - IP level load balancing inside the linux kernel, it can direct requests to TCP or UDP based services to the real servers

IP load balancing methods:
VS via NAT ( use the -m option)
requests and response packets pass through the LB , the lb can become bottleneck at more than 20 servers

VS via IP tunneling (use the -i option)
lb schedulles the requests to the cluster servers, responses are send directly from the cluster servers
servers must have ip tunneling ( IP encapsulation ) protocol enabled

VS via direct routing (use the -g option)
virtual IP is shared by the LB and the servers, each server has an interface with the virtual IP
servers and lb must have one of their ifaces linked by a switch or hub.

VS local node(set by the kernel , not by ipvsadm)
it is used to test LVS on a single machine


Schedulling algorithms:
round robin (rr)
weighted round robin (wrr)
least-connection (lc)
weighted least-connection (wlc)
locality-based least-connection (lblc)
locality-based least-connection with replication (lblcr)
destination hashing (dh)
source hashing (sh)
shortest expected delay (sed)
never queue (nq)


1. Load ipvs kernel module
modprobe ip_vs

2. Enable ip forwarding and ip nonlocal bind
net.ipv4.ip_forward = 1
net.ipv4.ip_nonlocal_bind = 1

3. Install ipvsadm package:
#centos
yum install ipvsadm
#ubuntu
apt-get install ipvsadm

4. Start ipvsadm service
#centos
touch /etc/sysconfig/ipvsadm

systemctl enable ipvsadm && systemctl start ipvsadm

5. Clear the existing ipvs table
ipvsadm -C

6. Add the virtual service amd specify schedulling algorithm
ipvsadm -A -t 192.168.1.12:80 -s rr

7. Add real servers to a viartual service and specify packet forwarding method
ipvsadm -a -t 192.168.1.12:80 -r 192.168.1.1:80 -m
ipvsadm -a -t 192.168.1.12:80 -r 192.168.1.2:80 -m

8. List viarual server table
ipvsadm -l -n


## Config file locations
/etc/sysconfig/ipvsadm-config (centos)
/etc/default/ipvsadm (debian)



##################### keepalived and ldirectord #######################

# keepalived manual installation

1. Install development tools
yum group install 'Development Tools'
apt install build-essential libssl-dev

2. Download the package
wget http://www.keepalived.org/software/keepalived-2.0.7.tar.gz

3. Extract archive

4. Build the binaries
./configure
make && make install


# ldirectord is a service daemon that was created to monitor and administer the real servers in LVS. The daemon perodically monitors and the real servers and if any of them is failed
it is removed from the pool

1. Install the package
apt-get install ldirectod

2. add config file
vi /etc/ha.d/ldirectord.cf

#sample config

checktimeout=3
checkinterval=5
autoreload=yes
logfile="/var/log/ldirectord.log"
quiescent=no
virtual=192.168.1.10:80
  real=192.168.1.1:80 masq
  real=192.168.1.2:80 masq
  scheduler=rr
  protocol=tcp
  checktype=connect
  checkport=80


## Server state sync daemon (syncd)
supports connection synchronization from the primary lb to the backup lb though multicast

1. Run on the master lb
ipvsadm --start-daemon=master --mcast-interface=eth0

2. run on the backup lb
ipvsadm --start-daemon=backup --mcast-interface=eth0

#to stop daemon
ipvsadm --stop-daemon
